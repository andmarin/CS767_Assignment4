{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and reduce size of dataset to allow flexibility in improving performance through hyper-parameter value changes\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.linalg import sqrtm\n",
    "import time\n",
    "\n",
    "# Reduce dataset size\n",
    "def reduce_dataset(dataset, num_samples=10000):\n",
    "    return dataset.take(num_samples)\n",
    "\n",
    "# Load dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "reduced_train_dataset = reduce_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up to run the baseline model on reduced dataset with same hyperparameter values\n",
    "\n",
    "\"\"\"\n",
    "# Generator model definition\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(100,)),  # Explicitly define input shape\n",
    "        layers.Dense(7 * 7 * 256, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator model definition\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),  # Explicitly define input shape\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    # Ensure real_images and generated_images are in correct shape\n",
    "    if real_images.shape[-1] != 3:\n",
    "        real_images = tf.image.grayscale_to_rgb(real_images)  # Convert grayscale to RGB\n",
    "    if generated_images.shape[-1] != 3:\n",
    "        generated_images = tf.image.grayscale_to_rgb(generated_images)  # Convert grayscale to RGB\n",
    "\n",
    "    # Resize images to (75, 75, 3)\n",
    "    real_resized = tf.image.resize(real_images, (75, 75))\n",
    "    generated_resized = tf.image.resize(generated_images, (75, 75))\n",
    "\n",
    "    # Extract features using InceptionV3\n",
    "    inception_model = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(75, 75, 3))\n",
    "    real_act = inception_model.predict(real_resized, verbose=0)\n",
    "    generated_act = inception_model.predict(generated_resized, verbose=0)\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu_real, sigma_real = np.mean(real_act, axis=0), np.cov(real_act, rowvar=False)\n",
    "    mu_generated, sigma_generated = np.mean(generated_act, axis=0), np.cov(generated_act, rowvar=False)\n",
    "\n",
    "    # Calculate FID score\n",
    "    diff = mu_real - mu_generated\n",
    "    covmean = scipy.linalg.sqrtm(sigma_real.dot(sigma_generated), disp=False)[0]\n",
    "\n",
    "    # Numerical stability check\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid_score = diff.dot(diff) + np.trace(sigma_real + sigma_generated - 2 * covmean)\n",
    "    return fid_score\n",
    "\n",
    "# Training function\n",
    "def train_and_evaluate(train_dataset, epochs, noise_dim, batch_size, generator_fn, discriminator_fn, fid_batch_size=1000):\n",
    "    generator = generator_fn()\n",
    "    discriminator = discriminator_fn()\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_images in train_dataset.batch(batch_size):  # Ensure correct batching\n",
    "            noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                generated_images = generator(noise, training=True)\n",
    "                real_output = discriminator(real_images, training=True)\n",
    "                fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "                gen_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
    "                disc_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output) + \\\n",
    "                            tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "            gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "            gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "            generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid_noise = tf.random.normal([fid_batch_size, noise_dim])\n",
    "    generated_images = generator(fid_noise, training=False)\n",
    "\n",
    "    # Ensure real_images has a batch dimension\n",
    "    real_images = tf.concat([batch for batch in train_dataset.take(fid_batch_size // batch_size)], axis=0)\n",
    "    if len(real_images.shape) == 3:  # Add batch dimension if missing\n",
    "        real_images = tf.expand_dims(real_images, axis=0)\n",
    "\n",
    "    # Convert to RGB and resize\n",
    "    real_images_resized = tf.image.grayscale_to_rgb(real_images)\n",
    "    real_images_resized = tf.image.resize(real_images_resized, (75, 75))\n",
    "\n",
    "    # Ensure real_images_resized has a batch dimension\n",
    "    if len(real_images_resized.shape) == 3:  # Add batch dimension if missing\n",
    "        real_images_resized = tf.expand_dims(real_images_resized, axis=0)\n",
    "\n",
    "    # Prepare generated images\n",
    "    generated_images = generator(tf.random.normal([fid_batch_size, noise_dim]), training=False)\n",
    "    if len(generated_images.shape) == 3:  # Add batch dimension if missing\n",
    "        generated_images = tf.expand_dims(generated_images, axis=0)\n",
    "\n",
    "    # Convert to RGB and resize\n",
    "    generated_images_resized = tf.image.grayscale_to_rgb(generated_images)\n",
    "    generated_images_resized = tf.image.resize(generated_images_resized, (75, 75))\n",
    "\n",
    "    print(\"Shape of real_images_resized:\", real_images_resized.shape)\n",
    "    print(\"Shape of generated_images_resized:\", generated_images_resized.shape)\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid_score = calculate_fid(real_images_resized, generated_images_resized)\n",
    "    return fid_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of real_images_resized: (1, 75, 75, 3)\n",
      "Shape of generated_images_resized: (1000, 75, 75, 3)\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 0us/step\n",
      "Baseline FID: 2339.4321782401666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBaseline FID: 2409.122352152087\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run baseline model and output FID score\n",
    "baseline_fid = train_and_evaluate(reduced_train_dataset, epochs=5, noise_dim=100, batch_size=64,\n",
    "                                  generator_fn=make_generator_model, discriminator_fn=make_discriminator_model)\n",
    "print(f\"Baseline FID: {baseline_fid}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Baseline FID: 2409.122352152087\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create model and functions with slightly different setup to handle varied hyper-parameter values\n",
    "\"\"\"\n",
    "\n",
    "# Generator model definition with variable noise_dim\n",
    "def make_generator_model(noise_dim=100):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(noise_dim,)),  # Use the noise_dim parameter here\n",
    "        layers.Dense(7 * 7 * 256, use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator model definition\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(28, 28, 1)),  # Explicitly define input shape\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    # Ensure real_images and generated_images are in correct shape\n",
    "    if real_images.shape[-1] != 3:\n",
    "        real_images = tf.image.grayscale_to_rgb(real_images)  # Convert grayscale to RGB\n",
    "    if generated_images.shape[-1] != 3:\n",
    "        generated_images = tf.image.grayscale_to_rgb(generated_images)  # Convert grayscale to RGB\n",
    "\n",
    "    # Resize images to (75, 75, 3)\n",
    "    real_resized = tf.image.resize(real_images, (75, 75))\n",
    "    generated_resized = tf.image.resize(generated_images, (75, 75))\n",
    "\n",
    "    # Extract features using InceptionV3\n",
    "    inception_model = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(75, 75, 3))\n",
    "    real_act = inception_model.predict(real_resized, verbose=0)\n",
    "    generated_act = inception_model.predict(generated_resized, verbose=0)\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu_real, sigma_real = np.mean(real_act, axis=0), np.cov(real_act, rowvar=False)\n",
    "    mu_generated, sigma_generated = np.mean(generated_act, axis=0), np.cov(generated_act, rowvar=False)\n",
    "\n",
    "    # Calculate FID score\n",
    "    diff = mu_real - mu_generated\n",
    "    covmean = scipy.linalg.sqrtm(sigma_real.dot(sigma_generated), disp=False)[0]\n",
    "\n",
    "    # Numerical stability check\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid_score = diff.dot(diff) + np.trace(sigma_real + sigma_generated - 2 * covmean)\n",
    "    return fid_score\n",
    "\n",
    "# Training function\n",
    "def train_and_evaluate(train_dataset, epochs, noise_dim, batch_size, generator_fn, discriminator_fn, fid_batch_size=1000):\n",
    "    generator = generator_fn(noise_dim=noise_dim)  # Pass noise_dim to generator\n",
    "    discriminator = discriminator_fn()\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_images in train_dataset.batch(batch_size):  # Ensure correct batching\n",
    "            noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                generated_images = generator(noise, training=True)\n",
    "                real_images = tf.reshape(real_images, (-1, 28, 28, 1))\n",
    "                real_output = discriminator(real_images, training=True)\n",
    "                fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "                gen_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
    "                disc_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output) + \\\n",
    "                            tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "            gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "            gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "            generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid_noise = tf.random.normal([fid_batch_size, noise_dim])\n",
    "    generated_images = generator(fid_noise, training=False)\n",
    "\n",
    "    # Collect real_images properly\n",
    "    real_images = []\n",
    "    for batch in train_dataset.take(fid_batch_size // batch_size):\n",
    "        real_images.append(batch)\n",
    "    real_images = tf.concat(real_images, axis=0)\n",
    "\n",
    "    # Convert grayscale to RGB\n",
    "    real_images_resized = tf.image.grayscale_to_rgb(real_images)\n",
    "    real_images_resized = tf.image.resize(real_images_resized, (75, 75))\n",
    "\n",
    "    # Prepare generated images\n",
    "    generated_images_resized = tf.image.grayscale_to_rgb(generated_images)\n",
    "    generated_images_resized = tf.image.resize(generated_images_resized, (75, 75))\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid_score = calculate_fid(real_images_resized, generated_images_resized)\n",
    "    return fid_score\n",
    "\n",
    "# Parameter sweep\n",
    "def parameter_sweep(train_dataset, generator_fn, discriminator_fn):\n",
    "    results = []\n",
    "\n",
    "    learning_rates = [1e-4, 2e-4]\n",
    "    noise_dims = [100, 150]\n",
    "    batch_sizes = [64, 128]\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for noise_dim in noise_dims:\n",
    "            for batch_size in batch_sizes:\n",
    "                dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "                #print(f\"Training with lr={lr}, noise_dim={noise_dim}, batch_size={batch_size}...\")\n",
    "                fid_score = train_and_evaluate(dataset, epochs=5, noise_dim=noise_dim, batch_size=batch_size,\n",
    "                                               generator_fn=generator_fn, discriminator_fn=discriminator_fn)\n",
    "                #print(f\"FID score: {fid_score}\")\n",
    "\n",
    "                results.append((lr, noise_dim, batch_size, fid_score))\n",
    "\n",
    "    # Sort results by FID in ascending order (lower is better) and display the top 5\n",
    "    results = sorted(results, key=lambda x: x[3])[:5]\n",
    "    print(\"\\nTop 5 results:\")\n",
    "    for result in results:\n",
    "        print(f\"Learning Rate: {result[0]}, Noise Dim: {result[1]}, Batch Size: {result[2]}, FID Score: {result[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parameter sweep function to see which hyper-parameter values returned the lowest FID score (lower score is better)\n",
    "parameter_sweep(reduced_train_dataset, make_generator_model, make_discriminator_model)\n",
    "\n",
    "\"\"\"\n",
    "Results shared below (Originally run in Google Colab)\n",
    "Top 5 results below:\n",
    "Learning Rate: 0.0001, Noise Dim: 150, Batch Size: 64, FID Score: 942.9753430657199\n",
    "Learning Rate: 0.0001, Noise Dim: 150, Batch Size: 128, FID Score: 954.4840038581231\n",
    "Learning Rate: 0.0001, Noise Dim: 100, Batch Size: 64, FID Score: 1006.7283447952834\n",
    "Learning Rate: 0.0002, Noise Dim: 150, Batch Size: 128, FID Score: 1010.5490313632524\n",
    "Learning Rate: 0.0002, Noise Dim: 150, Batch Size: 64, FID Score: 1053.3903247668918\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "Learning Rate: 0.0001, Noise Dim: 150, Batch Size: 64, FID Score: 891.9216246410855\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attempt to improve model architecture with best parameters\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "# Generator model with enhanced architecture\n",
    "def make_generator_model_v2(noise_dim=100):\n",
    "    inputs = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(7 * 7 * 256, use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((7, 7, 256))(x)\n",
    "\n",
    "    # Residual block 1\n",
    "    skip1 = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(skip1)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    # Residual block 2\n",
    "    skip2 = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(skip2)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    # Skip connection for residual learning\n",
    "    x = layers.Add()([x, skip2])  # Adding skip connection\n",
    "\n",
    "    # Output layer\n",
    "    outputs = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Discriminator model with spectral normalization\n",
    "def make_discriminator_model_v2():\n",
    "    def spectral_norm(layer):\n",
    "        # For Spectral Normalization, you can use the standard layer or add custom logic if required.\n",
    "        # This placeholder just returns the layer itself for simplicity.\n",
    "        return layer\n",
    "\n",
    "    inputs = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(inputs)\n",
    "    x = spectral_norm(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = spectral_norm(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(1)(x)  # No activation for logits\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# FID score calculation\n",
    "def calculate_fid(real_images, generated_images):\n",
    "    if real_images.shape[-1] != 3:\n",
    "        real_images = tf.image.grayscale_to_rgb(real_images)  # Convert grayscale to RGB\n",
    "    if generated_images.shape[-1] != 3:\n",
    "        generated_images = tf.image.grayscale_to_rgb(generated_images)  # Convert grayscale to RGB\n",
    "\n",
    "    real_resized = tf.image.resize(real_images, (75, 75))\n",
    "    generated_resized = tf.image.resize(generated_images, (75, 75))\n",
    "\n",
    "    inception_model = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(75, 75, 3))\n",
    "    real_act = inception_model.predict(real_resized, verbose=0)\n",
    "    generated_act = inception_model.predict(generated_resized, verbose=0)\n",
    "\n",
    "    mu_real, sigma_real = np.mean(real_act, axis=0), np.cov(real_act, rowvar=False)\n",
    "    mu_generated, sigma_generated = np.mean(generated_act, axis=0), np.cov(generated_act, rowvar=False)\n",
    "\n",
    "    diff = mu_real - mu_generated\n",
    "    covmean = scipy.linalg.sqrtm(sigma_real.dot(sigma_generated), disp=False)[0]\n",
    "\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid_score = diff.dot(diff) + np.trace(sigma_real + sigma_generated - 2 * covmean)\n",
    "    return fid_score\n",
    "\n",
    "# Training function\n",
    "def train_and_evaluate(train_dataset, epochs, noise_dim, batch_size, generator_fn, discriminator_fn, fid_batch_size=1000):\n",
    "    generator = generator_fn(noise_dim=noise_dim)\n",
    "    discriminator = discriminator_fn()\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_images in train_dataset.batch(batch_size):\n",
    "            noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                generated_images = generator(noise, training=True)\n",
    "                real_images = tf.reshape(real_images, (-1, 28, 28, 1))\n",
    "                real_output = discriminator(real_images, training=True)\n",
    "                fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "                gen_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
    "                disc_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output) + \\\n",
    "                            tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "            gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "            gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "            generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "\n",
    "    fid_noise = tf.random.normal([fid_batch_size, noise_dim])\n",
    "    generated_images = generator(fid_noise, training=False)\n",
    "\n",
    "    real_images = []\n",
    "    for batch in train_dataset.take(fid_batch_size // batch_size):\n",
    "        real_images.append(batch)\n",
    "    real_images = tf.concat(real_images, axis=0)\n",
    "\n",
    "    real_images_resized = tf.image.grayscale_to_rgb(real_images)\n",
    "    real_images_resized = tf.image.resize(real_images_resized, (75, 75))\n",
    "    generated_images_resized = tf.image.grayscale_to_rgb(generated_images)\n",
    "    generated_images_resized = tf.image.resize(generated_images_resized, (75, 75))\n",
    "\n",
    "    fid_score = calculate_fid(real_images_resized, generated_images_resized)\n",
    "    return fid_score\n",
    "\n",
    "# Parameter sweep\n",
    "def parameter_sweep(train_dataset, generator_fn, discriminator_fn):\n",
    "    results = []\n",
    "\n",
    "    learning_rates = [1e-4]\n",
    "    noise_dims = [150]\n",
    "    batch_sizes = [64]\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for noise_dim in noise_dims:\n",
    "            for batch_size in batch_sizes:\n",
    "                dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "                fid_score = train_and_evaluate(dataset, epochs=10, noise_dim=noise_dim, batch_size=batch_size,\n",
    "                                               generator_fn=generator_fn, discriminator_fn=discriminator_fn)\n",
    "                results.append((lr, noise_dim, batch_size, fid_score))\n",
    "\n",
    "    print(\"\\nResult:\")\n",
    "    for result in results:\n",
    "        print(f\"Learning Rate: {result[0]}, Noise Dim: {result[1]}, Batch Size: {result[2]}, FID Score: {result[3]}\")\n",
    "\n",
    "# Execute parameter sweep\n",
    "parameter_sweep(reduced_train_dataset, make_generator_model_v2, make_discriminator_model_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
